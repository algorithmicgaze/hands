{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training on Pose Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4090 (UUID: GPU-59ba7a4d-461d-6c44-7eea-a4200c322183)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_line(lines, prefix):\n",
    "  for line in lines:\n",
    "    if line.startswith('Frames:'):\n",
    "      return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bvh_file(file_path):\n",
    "    # Read file contents\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_contents = f.read()\n",
    "        \n",
    "    # Split file contents by newline characters\n",
    "    lines = file_contents.split('\\n')\n",
    "\n",
    "    # Find the channel names\n",
    "    channel_names = []\n",
    "    joint_name = None\n",
    "    for line in lines:\n",
    "      line = line.strip()\n",
    "      if line.startswith('JOINT') or line.startswith('ROOT'):\n",
    "        # Joint line looks like this:\n",
    "        # JOINT Spine2\n",
    "        joint_data = line.split(' ')\n",
    "        joint_name = joint_data[1]\n",
    "      if line.startswith('CHANNELS'):\n",
    "        # Channels line looks like this:\n",
    "        # CHANNELS 3 Yrotation Xrotation Zrotation\n",
    "        channel_data = line.split(' ')\n",
    "        for channel in channel_data[2:]:\n",
    "          channel_names.append(f'{joint_name}_{channel}')\n",
    "    \n",
    "    # Find the number of frames and the start of the motion data\n",
    "    num_frames_line = find_line(lines, 'Frames:')\n",
    "    num_frames = int(num_frames_line.split(' ')[1])\n",
    "    motion_data_index = lines.index('MOTION') + 3\n",
    "    header = '\\n'.join(lines[:motion_data_index])\n",
    "    \n",
    "    # Find the number of channels in the file\n",
    "    first_frame_data = lines[motion_data_index].strip().split(' ')\n",
    "    num_channels = len(first_frame_data)\n",
    "    print('Channels:', num_channels)\n",
    "\n",
    "    # Extract the motion data as a string\n",
    "    motion_data_str = ''.join(lines[motion_data_index:motion_data_index+num_frames])\n",
    "    \n",
    "    # Convert the motion data to a numpy array\n",
    "    motion_data = np.fromstring(motion_data_str, sep=' ')\n",
    "    motion_data = motion_data.reshape((num_frames, -1))\n",
    "    \n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    motion_tensor = torch.tensor(motion_data, dtype=torch.float32)\n",
    "    \n",
    "    return motion_tensor, header, channel_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels: 183\n",
      "torch.Size([19298, 183])\n"
     ]
    }
   ],
   "source": [
    "raw_data, header, channel_names = read_bvh_file('train_data/flute2.bvh')\n",
    "print(raw_data.shape)\n",
    "#print(channel_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BVHDataset(Dataset):\n",
    "    def __init__(self, file_path, input_size, output_size, seq_length, future_delta):\n",
    "        self.file_path = file_path\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_length = seq_length\n",
    "        self.future_delta = future_delta\n",
    "\n",
    "        # Read BVH file\n",
    "        self.motion_tensor, self.header, self.channel_names = read_bvh_file(file_path)\n",
    "\n",
    "        # Compute the total number of sequences in the file\n",
    "        self.total_sequences = len(self.motion_tensor) - self.seq_length - self.future_delta\n",
    "\n",
    "        # Compute input_mean and input_std\n",
    "        self.input_mean = torch.mean(self.motion_tensor, dim=(0,))\n",
    "        self.input_std = torch.std(self.motion_tensor, dim=(0,))\n",
    "        self.input_std = torch.where(self.input_std == 0, torch.tensor(1e-7), self.input_std)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Compute the sequence index for the given index\n",
    "        seq_idx = idx + self.seq_length\n",
    "\n",
    "        # Get the sequence of length seq_length as input x\n",
    "        input_tensor = self.motion_tensor[seq_idx - self.seq_length:seq_idx, :self.input_size]\n",
    "\n",
    "        # Get the frame future_delta frames into the future as output y\n",
    "        output_tensor = self.motion_tensor[seq_idx + self.future_delta, :self.output_size]\n",
    "\n",
    "        # Normalize input and output tensors\n",
    "        input_tensor = (input_tensor - self.input_mean) / self.input_std\n",
    "        output_tensor = (output_tensor - self.input_mean[:self.output_size]) / self.input_std[:self.output_size]\n",
    "\n",
    "        return input_tensor, output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels: 183\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "\n",
    "num_channels = raw_data.shape[-1]\n",
    "input_size = num_channels\n",
    "output_size = num_channels\n",
    "seq_length = 100\n",
    "future_delta = 200\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "dataset = BVHDataset('train_data/flute2.bvh', input_size, output_size, seq_length, future_delta)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PoseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim, sequence_length):\n",
    "        super(PoseTransformer, self).__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.position_encoding = nn.Parameter(torch.randn(sequence_length, 1, model_dim))\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=model_dim * 4,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        x = self.embedding(x) + self.position_encoding\n",
    "        # x shape: (sequence_length, batch_size, model_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "        # x shape: (batch_size, sequence_length, model_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        # Use only the last frame for prediction\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 183\n",
    "output_dim = 183\n",
    "model_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "sequence_length = 100\n",
    "\n",
    "# Create the model\n",
    "model = PoseTransformer(input_dim, model_dim, num_heads, num_layers, output_dim, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler (optional)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (input_seq, target_seq) in enumerate(tqdm(train_loader)):\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_seq = model(input_seq)\n",
    "        loss = criterion(output_seq, target_seq)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_seq, target_seq) in enumerate(val_loader):\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            output_seq = model(input_seq)\n",
    "            loss = criterion(output_seq, target_seq)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
